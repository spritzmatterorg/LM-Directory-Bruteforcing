{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import torchtext\n",
    "from anytree import Node\n",
    "import heapq\n",
    "from queue import LifoQueue\n",
    "import queue\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder='LM_datasets'\n",
    "train_df = pd.read_csv(os.path.join(data_folder, 'train.csv'))\n",
    "validation_df = pd.read_csv(os.path.join(data_folder, 'validation.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_folder, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique files in test set: 119\n"
     ]
    }
   ],
   "source": [
    "# Prepare test_df for evaluation\n",
    "test_df_list=[]\n",
    "\n",
    "# Create a different df for all the records of each unique filename\n",
    "for filename in test_df['Filename'].unique():\n",
    "    test_df_list.append(test_df[test_df['Filename']==filename])\n",
    "\n",
    "print('Number of unique files in test set:', len(test_df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_train=[]\n",
    "for filename in train_df['Filename'].unique():\n",
    "    df_list_train.append(train_df[train_df['Filename']==filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM model loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer to prepare the data\n",
    "\n",
    "year_token='YEAR'\n",
    "\n",
    "def custom_tokenizer(path,MAX_DEPTH):\n",
    "\n",
    "    # remove leading slash\n",
    "    path=path.lstrip('/')\n",
    "\n",
    "    # Split the path into words\n",
    "    path_words = path.split('/')\n",
    "\n",
    "    # Trim the path to MAX_DEPTH\n",
    "    path_words = path_words[:MAX_DEPTH]\n",
    "\n",
    "    #YEAR token substitution for 4-digit numbers\n",
    "    for i,tok in enumerate(path_words):\n",
    "        pattern = r'^\\d{4}$'\n",
    "        if re.match(pattern, tok):\n",
    "            path_words[i]=year_token\n",
    "\n",
    "    return path_words\n",
    "\n",
    "# Function to yield tokens from the DataFrame\n",
    "def yield_tokens(data_iter,MAX_DEPTH):\n",
    "    for path in data_iter:\n",
    "        yield custom_tokenizer(path, MAX_DEPTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "def create_vocabulary(MIN_FREQ,MAX_DEPTH):\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_df['Path'],MAX_DEPTH), min_freq=MIN_FREQ)\n",
    "    vocab.insert_token('<unk>', 0)\n",
    "    vocab.insert_token('<eos>', 2)\n",
    "    vocab.insert_token('<sos>', 1)\n",
    "    vocab.insert_token('<pad>',3)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    # print(f'len vocab = {len(vocab)}')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate,\n",
    "                tie_weights):\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.tie_weights = tie_weights\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Dropout between embedding and lstm\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "        # Dropout between lstm and fc\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
    "            self.embedding.weight = self.fc.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions, setting the number of predictions to be generated to prediction_limit\n",
    "\n",
    "def generate(tokens, MAX_DEPTH, model, custom_tokenizer, vocab, device,prediction_limit, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    # tokens = custom_tokenizer(prompt, MAX_DEPTH)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        src = torch.LongTensor([indices]).to(device)\n",
    "        prediction, hidden = model(src, hidden)\n",
    "        # Directly modify the prediction vector to set EOS probability to 0\n",
    "        eos_index = vocab['<eos>'] # As per your vocab setup\n",
    "        prediction[:, -1, eos_index] = -float('inf')\n",
    "\n",
    "        sos_index = vocab['<sos>'] # As per your vocab setup\n",
    "        prediction[:, -1, sos_index] = -float('inf')\n",
    "        \n",
    "        unk_index = vocab['<unk>']\n",
    "        prediction[:, -1, unk_index] = -float('inf')\n",
    "        \n",
    "        probs = torch.softmax(prediction[:, -1], dim=-1)\n",
    "        \n",
    "        top_k=prediction_limit\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "        # Create a list of tuples (prob, token)\n",
    "        tokens = vocab.get_itos()\n",
    "        prob_token_list = [(prob.item(), tokens[i]) for prob, i in zip(top_probs[0], top_indices[0])]\n",
    "        return prob_token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: model_MD10_MF3_es128_nl3_dr0.2_loss3.569985.pt, MAX_DEPTH: 10, MIN_FREQ: 3, embedding_size: 128, num_layers: 3, dropout_rate: 0.2, loss: 3.569985\n",
      "vocab_size = 40717\n",
      "MODEL: model_MD10_MF5_es128_nl3_dr0.4_loss3.285578.pt, MAX_DEPTH: 10, MIN_FREQ: 5, embedding_size: 128, num_layers: 3, dropout_rate: 0.4, loss: 3.285578\n",
      "vocab_size = 24507\n",
      "MODEL: model_MD5_MF3_es256_nl4_dr0.4_loss3.556982.pt, MAX_DEPTH: 5, MIN_FREQ: 3, embedding_size: 256, num_layers: 4, dropout_rate: 0.4, loss: 3.556982\n",
      "vocab_size = 36724\n",
      "MODEL: model_MD5_MF5_es512_nl4_dr0.6_loss3.271419.pt, MAX_DEPTH: 5, MIN_FREQ: 5, embedding_size: 512, num_layers: 4, dropout_rate: 0.6, loss: 3.271419\n",
      "vocab_size = 22170\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained models\n",
    "\n",
    "models_folder='saved_models'\n",
    "\n",
    "models=[]\n",
    "for file in os.listdir(models_folder):\n",
    "    model_state_dict=torch.load(f'saved_models/{file}',map_location=torch.device(device))\n",
    "    \n",
    "    # load hyperparameters from the filename\n",
    "    model_name_without_extension = file.split('.pt')[0]\n",
    "\n",
    "    # Split the model name by underscore to get the components\n",
    "    components = model_name_without_extension.split('_')\n",
    "\n",
    "    # Extract the hyperparameters\n",
    "    model_prefix = components[0] # This is usually the model type or identifier\n",
    "    MAX_DEPTH = int(components[1][2:]) # Extract the number after 'MD'\n",
    "    MIN_FREQ = int(components[2][2:]) # Extract the number after 'MF'\n",
    "    embedding_size = int(components[3][2:]) # Extract the number after 'es'\n",
    "    num_layers = int(components[4][2:]) # Extract the number after 'nl'\n",
    "    dropout_rate = float(components[5][2:]) # Extract the number after 'dr'\n",
    "    loss = float(components[6][4:])\n",
    "    vocab= create_vocabulary(MIN_FREQ,MAX_DEPTH)\n",
    "    print(f'MODEL: {file}, MAX_DEPTH: {MAX_DEPTH}, MIN_FREQ: {MIN_FREQ}, embedding_size: {embedding_size}, num_layers: {num_layers}, dropout_rate: {dropout_rate}, loss: {loss}')\n",
    "    print(f'vocab_size = {len(vocab)}')\n",
    "    vocab_size = len(vocab)\n",
    "    tie_weights = True\n",
    "\n",
    "    model = LSTM(vocab_size, embedding_size, embedding_size, num_layers, dropout_rate, tie_weights).to(device)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    models.append((file,model,vocab,MAX_DEPTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bruteforce algorithm evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree-builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tree\n",
    "def create_tree_with_occurences(df,root):\n",
    "    def get_or_create_node(name, parent):\n",
    "        children = parent.children if parent else []\n",
    "        for child in children:\n",
    "            if child.name == name:\n",
    "                child.count += 1\n",
    "                return child\n",
    "        return Node(name, parent=parent, count=1, depth=parent.depth+1)\n",
    "\n",
    "\n",
    "    paths=df['Path'].tolist()\n",
    "\n",
    "    for path in paths:\n",
    "        components = path.split(\"/\")\n",
    "        current_node = root\n",
    "        for component in components:\n",
    "            if component != \"\":\n",
    "                current_node = get_or_create_node(component, current_node)\n",
    "    return root\n",
    "\n",
    "def create_wordlist_tree(wordlist_file, train_root):\n",
    "    with open(wordlist_file, 'r') as file:\n",
    "        wordlist = [line.strip() for line in file]\n",
    "    root = Node(\"/\")\n",
    "    \n",
    "    def add_node(current_node, train_node):\n",
    "        # Convert train_node.children into a dictionary\n",
    "        children_dict = {child.name: child for child in train_node.children}\n",
    "\n",
    "        for word in wordlist:\n",
    "            if word in children_dict:\n",
    "                child = Node(word, parent=current_node, count=children_dict[word].count)\n",
    "                add_node(child, children_dict[word])\n",
    "\n",
    "    add_node(root, train_root)\n",
    "    return root\n",
    "\n",
    "# Create the tree\n",
    "def create_tree(df):\n",
    "    def get_or_create_node(name, parent=None):\n",
    "        children = parent.children if parent else []\n",
    "        for child in children:\n",
    "            if child.name == name:\n",
    "                return child\n",
    "        return Node(name, parent=parent)\n",
    "\n",
    "\n",
    "    paths=df['Path'].tolist()\n",
    "\n",
    "    root = Node(\"/\")\n",
    "    for path in paths:\n",
    "        components = path.split(\"/\")\n",
    "        current_node = root\n",
    "        for component in components:\n",
    "            if component != \"\":\n",
    "                current_node = get_or_create_node(component, current_node)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bruteforce depth-breadth algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_dirbuster(wordlist_file, root, recursive, depth_first, time_1_request, number_of_threads, request_limit):\n",
    "    with open(wordlist_file, 'r') as file:\n",
    "        wordlist = [line.strip() for line in file]\n",
    "\n",
    "    # wordlist.sort()\n",
    "    total_requests = 0\n",
    "    successful_responses = 0\n",
    "    failed_responses = 0\n",
    "    total_time = 0\n",
    "\n",
    "    total_requests_list = []\n",
    "    successful_responses_list = []\n",
    "    failed_responses_list = []\n",
    "\n",
    "    def simulate(word, node):\n",
    "        nonlocal total_requests, successful_responses, failed_responses, total_time\n",
    "        total_requests += 1\n",
    "        total_time += time_1_request\n",
    "        children_map = {c.name: c for c in node.children}\n",
    "        if word in children_map:\n",
    "            successful_responses += 1\n",
    "            if recursive:\n",
    "                return children_map[word]\n",
    "        else:\n",
    "            failed_responses += 1\n",
    "        return None\n",
    "    \n",
    "    q = queue.Queue()\n",
    "\n",
    "    if depth_first:\n",
    "        q = LifoQueue()\n",
    "        \n",
    "    for word in wordlist:\n",
    "        q.put((word, root))\n",
    "\n",
    "    while not q.empty() and total_requests < request_limit:\n",
    "        word, node = q.get()\n",
    "        new_node = simulate(word, node)\n",
    "        if new_node is not None:\n",
    "            for new_word in wordlist:\n",
    "                q.put((new_word, new_node))\n",
    "\n",
    "        if total_requests==1 or (total_requests-1) %20 == 0 :\n",
    "            # Append the values to the lists\n",
    "            total_requests_list.append(total_requests)\n",
    "            successful_responses_list.append(successful_responses)\n",
    "            failed_responses_list.append(failed_responses)\n",
    "\n",
    "    if number_of_threads > 1:\n",
    "        total_time /= number_of_threads\n",
    "    \n",
    "\n",
    "\n",
    "    return total_requests_list, successful_responses_list, failed_responses_list, total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability-based directory bruteforce algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_bruteforcer(train_root, root, recursive, depth_first, time_1_request, number_of_threads, request_limit):\n",
    "    total_requests = 0\n",
    "    successful_responses = 0\n",
    "    failed_responses = 0\n",
    "    total_time = 0\n",
    "\n",
    "    counter=0\n",
    "\n",
    "    total_requests_list = []\n",
    "    successful_responses_list = []\n",
    "    failed_responses_list = []\n",
    "\n",
    "    def simulate(word, node):\n",
    "        \n",
    "        nonlocal total_requests, successful_responses, failed_responses, total_time\n",
    "        total_requests += 1\n",
    "        \n",
    "        total_time += time_1_request\n",
    "        children_map = {c.name: c for c in node.children}\n",
    "        if word in children_map:\n",
    "            \n",
    "            successful_responses += 1\n",
    "            if recursive:\n",
    "                return children_map[word]\n",
    "        else:\n",
    "            failed_responses += 1\n",
    "        return None\n",
    "\n",
    "    # Create a max heap with the words in the training tree\n",
    "    heap = queue.Queue()\n",
    "    if depth_first:\n",
    "        heap = LifoQueue()\n",
    "    children_list=[]\n",
    "    for node in train_root.children:\n",
    "        children_list.append((-node.count / sum(c.count for c in train_root.children),node.name,random.randint(0,1000000), node, root))\n",
    "    children_list.sort(key=lambda x: x[1])\n",
    "    \n",
    "    for child in children_list:\n",
    "        heap.put(child)\n",
    "\n",
    "    while not heap.empty() and total_requests < request_limit:\n",
    "        \n",
    "        _,word,_,node_word,target=heap.get()\n",
    "        \n",
    "        new_node = simulate(word, target)\n",
    "        if new_node is not None:\n",
    "            children_list=[]\n",
    "            # Add the children of the newly found directory to the heap\n",
    "            for child in node_word.children:\n",
    "                children_list.append((-child.count / sum(c.count for c in node_word.children),child.name,random.randint(0, 1000000), child, new_node))\n",
    "            \n",
    "            children_list.sort(key=lambda x: x[1])\n",
    "\n",
    "            for child in children_list:\n",
    "                heap.put(child)\n",
    "            counter+=len(node_word.children)\n",
    "        if total_requests==1 or (total_requests-1) %20 == 0 :\n",
    "            # Append the values to the lists\n",
    "            total_requests_list.append(total_requests)\n",
    "            successful_responses_list.append(successful_responses)\n",
    "            failed_responses_list.append(failed_responses)\n",
    "\n",
    "    if number_of_threads > 1:\n",
    "        total_time /= number_of_threads\n",
    "    \n",
    "    return total_requests_list, successful_responses_list, failed_responses_list, total_time, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_bruteforcer(train_root, root, recursive, time_1_request, number_of_threads, request_limit,wordlist_file):\n",
    "    with open(wordlist_file, 'r') as file:\n",
    "        wordlist = [line.strip() for line in file]\n",
    "    found_nodes=[root]\n",
    "\n",
    "    total_requests = 0\n",
    "    successful_responses = 0\n",
    "    failed_responses = 0\n",
    "    total_time = 0\n",
    "\n",
    "    counter=0\n",
    "    \n",
    "    total_requests_list = []\n",
    "    successful_responses_list = []\n",
    "    failed_responses_list = []\n",
    "\n",
    "    not_redundant_tuples=[]\n",
    "    def simulate(word, node):\n",
    "        \n",
    "        nonlocal total_requests, successful_responses, failed_responses, total_time, found_nodes, not_redundant_tuples\n",
    "        total_requests += 1\n",
    "        total_time += time_1_request\n",
    "        children_map = {c.name: c for c in node.children}\n",
    "        if word in children_map:\n",
    "            found_nodes.append(children_map[word])\n",
    "            not_redundant_tuples.append((node,children_map[word]))\n",
    "            successful_responses += 1\n",
    "            if recursive:\n",
    "                \n",
    "                return children_map[word]\n",
    "        else:\n",
    "            failed_responses += 1\n",
    "        return None\n",
    "    # Create a max heap with the words in the training tree\n",
    "    sum_to_add=sum(c.count for c in train_root.children)\n",
    "    sum_to_add=max(1,sum_to_add)\n",
    "    heap = [(-node.count / sum_to_add,node.name,random.randint(0,1000000), node, root) for node in train_root.children]\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    counter=len(heap)\n",
    "\n",
    "    while len(heap)!=0 and total_requests < request_limit:\n",
    "        \n",
    "        _,word,_,node_word,target=heapq.heappop(heap)\n",
    "        \n",
    "        new_node = simulate(word, target)\n",
    "        if new_node is not None:\n",
    "            # Add the children of the newly found directory to the heap\n",
    "            for child in node_word.children:\n",
    "                sum_to_add=sum(c.count for c in node_word.children)\n",
    "                sum_to_add=max(1,sum_to_add)\n",
    "                heapq.heappush(heap, (-child.count / sum_to_add,child.name,random.randint(0,1000000), child, new_node))\n",
    "            counter+=len(node_word.children)\n",
    "        if total_requests==1 or (total_requests-1) %20 == 0 :\n",
    "            # Append the values to the lists\n",
    "            total_requests_list.append(total_requests)\n",
    "            successful_responses_list.append(successful_responses)\n",
    "            failed_responses_list.append(failed_responses)\n",
    "    wordlist_index=0\n",
    "    node_index=0\n",
    "    while total_requests < request_limit:\n",
    "        # print('enter_the_loop')\n",
    "        if wordlist_index >= len(wordlist):\n",
    "            wordlist_index=0\n",
    "            node_index+=1\n",
    "        if node_index >= len(found_nodes):\n",
    "            break\n",
    "        \n",
    "        children_map = {c.name: c for c in found_nodes[node_index].children}\n",
    "        if wordlist[wordlist_index] in children_map and (found_nodes[node_index],children_map[wordlist[wordlist_index]]) in not_redundant_tuples:\n",
    "            wordlist_index+=1\n",
    "            continue\n",
    "        \n",
    "        total_requests += 1\n",
    "        total_time += time_1_request\n",
    "        if wordlist[wordlist_index] in children_map:\n",
    "            successful_responses += 1\n",
    "            if recursive:\n",
    "                found_nodes.append(children_map[wordlist[wordlist_index]])\n",
    "        else:\n",
    "            failed_responses += 1\n",
    "        if total_requests==1 or (total_requests-1) %20 == 0 :\n",
    "            # Append the values to the lists\n",
    "            total_requests_list.append(total_requests)\n",
    "            successful_responses_list.append(successful_responses)\n",
    "            failed_responses_list.append(failed_responses)\n",
    "        wordlist_index+=1\n",
    "\n",
    "    if number_of_threads > 1:\n",
    "        total_time /= number_of_threads\n",
    "    \n",
    "    return total_requests_list, successful_responses_list, failed_responses_list, total_time, counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training tree and wordlist tree to be used for probability based attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38abd00a012a42fcb73a18f9513197aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DataFrames to train:   0%|          | 0/419 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the training tree\n",
    "train_root = Node(\"/\", count=1,depth=0)\n",
    "for df in tqdm(df_list_train, desc=\"DataFrames to train\"):\n",
    "    train_root = create_tree_with_occurences(df,train_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51bf30a3f3f4a9fa49490151ef82500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Wordlist Files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordlist_trees = []\n",
    "for wordlist_file in tqdm(os.listdir('chosen_wordlists'), desc=\"Wordlist Files\"):\n",
    "    wordlist_trees.append(create_wordlist_tree('chosen_wordlists/'+wordlist_file, train_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LM based algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_bruteforcer(model,vocab,MAX_DEPTH, root, recursive, time_1_request, number_of_threads, request_limit,prediction_limit):\n",
    "    total_requests = 0\n",
    "    successful_responses = 0\n",
    "    failed_responses = 0\n",
    "    total_time = 0\n",
    "\n",
    "    total_requests_list = []\n",
    "    successful_responses_list = []\n",
    "    failed_responses_list = []\n",
    "\n",
    "    def simulate(word, node):\n",
    "        \n",
    "        nonlocal total_requests, successful_responses, failed_responses, total_time\n",
    "        total_requests += 1\n",
    "        \n",
    "        total_time += time_1_request\n",
    "        children_map={}\n",
    "        for c in node.children:\n",
    "            name=c.name\n",
    "            pattern = r'^\\d{4}$'\n",
    "            if re.match(pattern, name):\n",
    "                name=year_token\n",
    "            children_map[name]=c\n",
    "            \n",
    "        if word in children_map:\n",
    "            successful_responses += 1\n",
    "            if recursive:\n",
    "                # return the node with the word associated with it\n",
    "                return word,children_map[word]\n",
    "        else:\n",
    "            failed_responses += 1\n",
    "        return None, None\n",
    "    # Create a max heap with the starting prediction of the model\n",
    "    # random.randint(0,10000)\n",
    "    predictions=generate(['<sos>'], MAX_DEPTH, model, custom_tokenizer, vocab, device,prediction_limit)\n",
    "    # print(predictions)\n",
    "    heap = [(-prob,1,word,['<sos>'], root) for prob,word in predictions]\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    while len(heap)!=0 and total_requests < request_limit:\n",
    "        \n",
    "        _,_,word,token_list,target=heapq.heappop(heap)\n",
    "        \n",
    "        child_name, new_node = simulate(word, target)\n",
    "        if new_node is not None:\n",
    "            new_token_list=token_list+[child_name]\n",
    "            # Add the children of the newly found directory to the heap\n",
    "            predictions=generate(new_token_list, MAX_DEPTH, model, custom_tokenizer, vocab, device,prediction_limit)\n",
    "            \n",
    "            # print(f'token_list: {new_token_list}, first 10 predictions: {predictions[:10]}')\n",
    "            for prob,word in predictions:\n",
    "                heapq.heappush(heap, (-prob,len(new_token_list),word,new_token_list, new_node))\n",
    "        if total_requests==1 or (total_requests-1) %20 == 0 :\n",
    "            # Append the values to the lists\n",
    "            total_requests_list.append(total_requests)\n",
    "            successful_responses_list.append(successful_responses)\n",
    "            failed_responses_list.append(failed_responses)\n",
    "\n",
    "    if number_of_threads > 1:\n",
    "        total_time /= number_of_threads\n",
    "    \n",
    "    return total_requests_list, successful_responses_list, failed_responses_list, total_time, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE TEST WEBSITE TREES\n",
    "# Prepare test_df for evaluation\n",
    "test_df_list=[]\n",
    "\n",
    "# Create a different df for all the records of each unique filename\n",
    "for filename in test_df['Filename'].unique():\n",
    "    test_df_list.append(test_df[test_df['Filename']==filename])\n",
    "\n",
    "print('Number of unique files in test set:', len(test_df_list))\n",
    "\n",
    "test_roots=[]\n",
    "# Create test roots:\n",
    "for df in tqdm(test_df_list, desc=\"Test trees creation\"):\n",
    "    test_roots.append(create_tree(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN EVERY POSSIBLE SIMULATION!\n",
    "\n",
    "# Global parameters of the simulation\n",
    "#\n",
    "# Specify here the parameters of the simulation:\n",
    "#\n",
    "# - recursive: whether the attack methodology should be recursive or not (apply to depth-first and breadth-first)\n",
    "# - time_1_request: estimated time in ms that takes to send a request and to receive a response (used to estimate the execution time)\n",
    "# - number_of_threads: number of threads that will be used to simulate the attack (used to estimate the execution time)\n",
    "# - request_limit: maximum number of requests that will be sent during the simulation (used to estimate the execution time)\n",
    "#\n",
    "\n",
    "recursive = True\n",
    "time_1_request = 150\n",
    "number_of_threads = 20\n",
    "request_limit = 100000\n",
    "\n",
    "categories=list(train_df['Type'].unique()) + ['general']\n",
    "wordlist_names = [os.path.splitext(os.path.basename(file))[0] for file in os.listdir('chosen_wordlists')]\n",
    "\n",
    "\n",
    "dfs_toadd=[]\n",
    "\n",
    "for category in categories:\n",
    "    print(f'Category: {category}')\n",
    "    if category=='general':\n",
    "        print('creating trees')\n",
    "        df_list_train=[]\n",
    "        for filename in train_df['Filename'].unique():\n",
    "            df_list_train.append(train_df[train_df['Filename']==filename])  \n",
    "        \n",
    "        train_root = Node(\"/\", count=1,depth=0)\n",
    "        for df in df_list_train:\n",
    "            train_root = create_tree_with_occurences(df,train_root)\n",
    "        \n",
    "        test_df_list=[]\n",
    "\n",
    "        # Create a different df for all the records of each unique filename\n",
    "        for filename in test_df['Filename'].unique():\n",
    "            test_df_list.append(test_df[test_df['Filename']==filename])\n",
    "        \n",
    "        test_roots=[]\n",
    "        # Create test roots:\n",
    "        for df in test_df_list:\n",
    "            test_roots.append(create_tree(df))\n",
    "        print('done creating trees')\n",
    "    else:\n",
    "        print('creating trees')\n",
    "        local_train_df=train_df[train_df['Type']==category]\n",
    "        local_test_df=test_df[test_df['Type']==category]\n",
    "\n",
    "        df_list_train=[]\n",
    "        for filename in local_train_df['Filename'].unique():\n",
    "            df_list_train.append(local_train_df[local_train_df['Filename']==filename])  \n",
    "        \n",
    "        train_root = Node(\"/\", count=1,depth=0)\n",
    "        for df in df_list_train:\n",
    "            train_root = create_tree_with_occurences(df,train_root)\n",
    "        \n",
    "        test_df_list=[]\n",
    "\n",
    "        # Create a different df for all the records of each unique filename\n",
    "        for filename in local_test_df['Filename'].unique():\n",
    "            test_df_list.append(local_test_df[local_test_df['Filename']==filename])\n",
    "        \n",
    "        test_roots=[]\n",
    "        # Create test roots:\n",
    "        for df in test_df_list:\n",
    "            test_roots.append(create_tree(df))\n",
    "        print('done creating trees')\n",
    "    \n",
    "    wordlist_trees = []\n",
    "    for wordlist_file in os.listdir('chosen_wordlists'):\n",
    "        wordlist_trees.append(create_wordlist_tree('chosen_wordlists/'+wordlist_file, train_root))\n",
    "    for wordlist_name, wordlist_tree in tqdm(zip(wordlist_names, wordlist_trees), desc=\"Wordlist Trees\"):\n",
    "        for test_root in test_roots:\n",
    "            # Simulate the breadth approach\n",
    "            total_requests_list, successful_responses_list, failed_responses_list, total_time = simulate_dirbuster('chosen_wordlists/'+wordlist_name+'.txt', test_root, recursive, False, time_1_request, number_of_threads, request_limit)\n",
    "            total_request_list=np.array(total_requests_list)\n",
    "            successful_responses_list=np.array(successful_responses_list)\n",
    "            failed_responses_list=np.array(failed_responses_list)\n",
    "            records = np.column_stack((np.repeat(category, len(total_requests_list)), np.repeat('breadth', len(total_requests_list)), np.repeat(wordlist_name, len(total_requests_list)), total_requests_list, successful_responses_list, failed_responses_list))\n",
    "            dfs_toadd.append(pd.DataFrame(records, columns=['Dataset','Simulation', 'Wordlist', 'Total_requests', 'successful_responses', 'failed_responses']))\n",
    "            \n",
    "            # Simulate the depth approach\n",
    "            total_requests_list, successful_responses_list, failed_responses_list, total_time = simulate_dirbuster('chosen_wordlists/'+wordlist_name+'.txt', test_root, recursive, True, time_1_request, number_of_threads, request_limit)\n",
    "            total_request_list=np.array(total_requests_list)\n",
    "            successful_responses_list=np.array(successful_responses_list)\n",
    "            failed_responses_list=np.array(failed_responses_list)\n",
    "            records = np.column_stack((np.repeat(category, len(total_requests_list)), np.repeat('depth', len(total_requests_list)),np.repeat(wordlist_name, len(total_requests_list)), total_requests_list, successful_responses_list, failed_responses_list))\n",
    "            dfs_toadd.append(pd.DataFrame(records, columns=['Dataset','Simulation', 'Wordlist', 'Total_requests', 'successful_responses', 'failed_responses']))\n",
    "            \n",
    "            # Simulate probability approach\n",
    "            total_requests_list, successful_responses_list, failed_responses_list, total_time, counter = probability_bruteforcer(wordlist_tree, test_root, recursive, time_1_request, number_of_threads, request_limit,'chosen_wordlists/'+ wordlist_name+'.txt')\n",
    "            total_request_list=np.array(total_requests_list)\n",
    "            successful_responses_list=np.array(successful_responses_list)\n",
    "            failed_responses_list=np.array(failed_responses_list)\n",
    "            records = np.column_stack((np.repeat(category, len(total_requests_list)), np.repeat('probability', len(total_requests_list)),np.repeat(wordlist_name, len(total_requests_list)), total_requests_list, successful_responses_list, failed_responses_list))\n",
    "            dfs_toadd.append(pd.DataFrame(records, columns=['Dataset','Simulation', 'Wordlist', 'Total_requests', 'successful_responses', 'failed_responses']))\n",
    "\n",
    "    for test_root in tqdm(test_roots, desc=\"probability\"):    \n",
    "        total_requests_list, successful_responses_list, failed_responses_list, total_time, counter = probability_bruteforcer(train_root, test_root, recursive, time_1_request, number_of_threads, request_limit,'chosen_wordlists/'+ wordlist_name+'.txt')\n",
    "        total_request_list=np.array(total_requests_list)\n",
    "        successful_responses_list=np.array(successful_responses_list)\n",
    "        failed_responses_list=np.array(failed_responses_list)\n",
    "        records = np.column_stack((np.repeat(category, len(total_requests_list)), np.repeat('probability', len(total_requests_list)) ,np.repeat('train-set', len(total_requests_list)), total_requests_list, successful_responses_list, failed_responses_list))\n",
    "        dfs_toadd.append(pd.DataFrame(records, columns=['Dataset','Simulation', 'Wordlist', 'Total_requests', 'successful_responses', 'failed_responses']))\n",
    "            \n",
    "\n",
    "    for test_root in tqdm(test_roots, desc=\"LM\"):\n",
    "        model_name, model, vocab, MAX_DEPTH = models[3]\n",
    "        # Simulate the LM approach\n",
    "        total_requests_list, successful_responses_list, failed_responses_list, total_time, counter = lm_bruteforcer(model,vocab, MAX_DEPTH, test_root, recursive, time_1_request, number_of_threads, request_limit,prediction_limit=750)\n",
    "        total_request_list=np.array(total_requests_list)\n",
    "        successful_responses_list=np.array(successful_responses_list)\n",
    "        failed_responses_list=np.array(failed_responses_list)\n",
    "        \n",
    "        records = np.column_stack((np.repeat(category, len(total_requests_list)), np.repeat('LM', len(total_requests_list)), np.repeat('general', len(total_requests_list)), total_requests_list, successful_responses_list, failed_responses_list))\n",
    "        dfs_toadd.append(pd.DataFrame(records, columns=['Dataset','Simulation', 'Wordlist', 'Total_requests', 'successful_responses', 'failed_responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE SIMULATIONS RESULTS IN CSV\n",
    "benchmark_df=pd.concat(dfs_toadd)\n",
    "benchmark_df.to_csv('benchmark_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS ON HOW MANY PREDICTIONS TO CONSIDER IN THE LM-BASED APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTACK SIMULATIONS USING DIFFERENT VALUES OF PREDICTIONS CONSIDERED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global parameters of the simulation\n",
    "recursive = True\n",
    "time_1_request = 150\n",
    "number_of_threads = 20\n",
    "request_limit = 100000\n",
    "\n",
    "dfs_toadd=[]\n",
    "\n",
    "param=[100, 250, 500, 750, 1000, 2000, 5000, 10000]\n",
    "for prediction_limit in tqdm(param):\n",
    "    for test_root in tqdm(test_roots, desc=\"LM simulations\"):\n",
    "        model_name, model, vocab, MAX_DEPTH = models[3]    \n",
    "        # Simulate the wordlist probability bruteforcer\n",
    "        total_requests_list, successful_responses_list, failed_responses_list, total_time, counter = lm_bruteforcer(model,vocab, MAX_DEPTH, test_root, recursive, time_1_request, number_of_threads, request_limit,prediction_limit)\n",
    "        total_request_list=np.array(total_requests_list)\n",
    "        successful_responses_list=np.array(successful_responses_list)\n",
    "        failed_responses_list=np.array(failed_responses_list)\n",
    "    \n",
    "        records = np.column_stack((np.repeat(prediction_limit, len(total_requests_list)), np.repeat(model_name, len(total_requests_list)), total_requests_list, successful_responses_list, failed_responses_list))\n",
    "        dfs_toadd.append(pd.DataFrame(records, columns=['Prediction_limit','Simulation', 'Total_requests', 'successful_responses', 'failed_responses']))\n",
    "param_validation_df = pd.concat(dfs_toadd, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_validation_df.to_csv('param_validation_results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_validation_df['Total_requests'] = param_validation_df['Total_requests'].astype(int)\n",
    "param_validation_df['successful_responses'] = param_validation_df['successful_responses'].astype(int)\n",
    "param_validation_df['failed_responses'] = param_validation_df['failed_responses'].astype(int)\n",
    "param_validation_df['Prediction_limit'] = param_validation_df['Prediction_limit'].astype(int)\n",
    "prediction_limits = param_validation_df['Prediction_limit'].unique()\n",
    "print(prediction_limits)\n",
    "\n",
    "\n",
    "# Create 1 figure where we plot the average successful responses for each prediction limit\n",
    "plt.figure(figsize=(7, 7), dpi=300)\n",
    "# Define a list of markers\n",
    "# Define a color palette\n",
    "palette = sns.color_palette(\"hsv\", len(prediction_limits))\n",
    "\n",
    "for i, prediction_limit in enumerate(prediction_limits):\n",
    "    df_simulation = param_validation_df[param_validation_df['Prediction_limit'] == prediction_limit]\n",
    "    avg_successful_responses = df_simulation.groupby('Total_requests')['successful_responses'].mean().reset_index()\n",
    "    \n",
    "    # Plot the average successful responses with a color from the colors list\n",
    "    plt.plot(avg_successful_responses['Total_requests'], avg_successful_responses['successful_responses'], label=f'topPredicts: {prediction_limit}')\n",
    "\n",
    "# plt.title('Average Successful Responses for different topPredict values')\n",
    "plt.xlabel('Total Requests')\n",
    "plt.ylabel('Successful Responses')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_limit_validation.pdf')\n",
    "plt.show()\n",
    "# plt.savefig('prediction_limit_validation.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
